{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Coding with tensorflow framework"]},{"cell_type":"markdown","metadata":{"id":"CnlHcYtz_rVc"},"source":["## Import Modules"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":507,"status":"ok","timestamp":1659747331129,"user":{"displayName":"quỳnh phạm","userId":"18182761465792935727"},"user_tz":-420},"id":"mBYrlNuDAIMt"},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-08-07 08:50:53.999907: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n","2022-08-07 08:50:53.999960: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"]}],"source":["import os\n","import pickle\n","import numpy as np\n","from tqdm.notebook import tqdm\n","\n","from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n","# from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.utils import to_categorical, plot_model\n","from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1659747331879,"user":{"displayName":"quỳnh phạm","userId":"18182761465792935727"},"user_tz":-420},"id":"xhzMfeskBePK"},"outputs":[],"source":["BASE_DIR = \"../data/\"\n","WORKING_DIR = \"../\"\n","MODEL_DIR = \"../trained_model/\""]},{"cell_type":"markdown","metadata":{"id":"HnFJqBPAG9wM"},"source":["## Extract Image Features"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6409,"status":"ok","timestamp":1659747524877,"user":{"displayName":"quỳnh phạm","userId":"18182761465792935727"},"user_tz":-420},"id":"t_4kSs_wBgG6","outputId":"bc937541-1d7a-4b82-fd16-df20082e6fab"},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-08-07 08:51:01.308855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-08-07 08:51:01.309292: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n","2022-08-07 08:51:01.309414: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n","2022-08-07 08:51:01.309478: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n","2022-08-07 08:51:01.309539: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n","2022-08-07 08:51:01.309600: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n","2022-08-07 08:51:01.309653: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n","2022-08-07 08:51:01.309707: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n","2022-08-07 08:51:01.309764: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n","2022-08-07 08:51:01.309773: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n","Skipping registering GPU devices...\n","2022-08-07 08:51:01.310292: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n","                                                                 \n"," block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n","                                                                 \n"," block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n","                                                                 \n"," block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n","                                                                 \n"," block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n","                                                                 \n"," block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n","                                                                 \n"," block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n","                                                                 \n"," block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n","                                                                 \n"," block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n","                                                                 \n"," block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n","                                                                 \n"," block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n","                                                                 \n"," block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n","                                                                 \n"," flatten (Flatten)           (None, 25088)             0         \n","                                                                 \n"," fc1 (Dense)                 (None, 4096)              102764544 \n","                                                                 \n"," fc2 (Dense)                 (None, 4096)              16781312  \n","                                                                 \n","=================================================================\n","Total params: 134,260,544\n","Trainable params: 134,260,544\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["# lan sau thu voi InceptionNet16\n","# load vgg16 model\n","# model = InceptionV3()\n","model = VGG16()\n","\n","# restructure the model\n","model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n","\n","# summary\n","model.summary()\n","\n","INPUT_WEIGHT = 224    # VGG16 -> 224 / 299 (InceptionV3)\n","INPUT_HEIGHT = 224    # VGG16 -> 224 / 299 (InceptionV3)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["da4428797a1745cd8812fd5680aeaa4c","0406ce95b3594820b977a0ead3d68b0f","3ca99a81cc344f438013c41289fc3f11","13d2fda0fc504c22923a8a32c01957aa","c05be3f3bda34c14a1c0864523e11b8b","6b7bdd13b2f948fa840cbe343c44943a","826eea88f1974155a13b572a9ac00ba9","95f4088b75014615bc4d68f678bb2bc5","59985f5377934236804742f982c64797","ab39904669e94113b8180fc6aa546fc0","749d77007d514161bb872967c1c4f7a2"]},"id":"1KjnKD3cBqFi","outputId":"b64ca0e7-a64f-4ffa-e81f-548bda72c374"},"outputs":[],"source":["# from ipywidgets import IntProgress\n","# extract feature from image\n","features = {}\n","directory = os.path.join(BASE_DIR, \"Images\")\n","\n","for img_name in tqdm(os.listdir(directory)):\n","  # load the image from file\n","  img_path = os.path.join(directory, img_name)\n","  image = load_img(img_path, target_size=(INPUT_WEIGHT, INPUT_HEIGHT))\n","  \n","  # convert image pixels to numpy array\n","  image = img_to_array(image)\n","\n","  # reshape data for model\n","  image = np.expand_dims(image, axis=0)\n","\n","  # preprocess image for vgg model\n","  image = preprocess_input(image)\n","\n","  # extract features\n","  feature = model.predict(image, verbose=0)\n","\n","  # get image ID\n","  image_id = img_name.split(\".\")[0]\n","\n","  # store feature\n","  features[image_id] = feature"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zPBYbshUSbrg"},"outputs":[],"source":["# store features in pickle\n","pickle.dump(features, open(os.path.join(BASE_DIR, \"features.pkl\"), \"wb\"))"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"hZImFsLMBvqK"},"outputs":[],"source":["# load features from pickle\n","with open(os.path.join(BASE_DIR, \"features.pkl\"), \"rb\") as f:\n","  features = pickle.load(f)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"TC2b6Xo0cjEa"},"outputs":[{"data":{"text/plain":["8091"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["len(features)"]},{"cell_type":"markdown","metadata":{"id":"OoRJVwnYVYfs"},"source":["## Load the captions data"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"wVLeNzSBVVW0"},"outputs":[],"source":["with open(os.path.join(BASE_DIR, \"captions.txt\"), \"r\") as f:\n","  next(f)\n","  captions_doc = f.read()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_pQxfq6Fc2A1"},"outputs":[],"source":["# create mapping of image to captions\n","mapping = {}\n","\n","i = 0\n","# process lines\n","for line in tqdm(captions_doc.split(\"\\n\")):\n","  # split the line by comma (,)\n","  tokens = line.split(',')\n","  if len(line) < 2: \n","    continue\n","\n","  image_id, caption = tokens[0].split('.')[0], tokens[1].replace(\" .\", \"\")\n","\n","  # create list if needed\n","  if image_id not in mapping:\n","    mapping[image_id] = []\n","  \n","  # store the caption\n","  mapping[image_id].append(caption)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UFalQX-sfCYG"},"outputs":[],"source":["len(mapping)"]},{"cell_type":"markdown","metadata":{"id":"mYOX-7CgiaJq"},"source":["## Preprocess Text Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"INy9JItnfTxi"},"outputs":[],"source":["def clean(mapping):\n","  for key, captions in mapping.items():\n","    for i in range(len(captions)):\n","      # take one caption at a time\n","      caption = captions[i]\n","\n","      # preprocessing steps\n","      # convert to lowercase\n","      caption = caption.lower()\n","\n","      # delete digits, special chars, ets ...\n","      caption = caption.replace('[^A-Za-z]', '')\n","\n","      # delete additional space\n","      caption = caption.replace('\\s+', ' ')\n","\n","      # add start and end tags to the caption\n","      caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word) > 1]) + \" endseq\"\n","\n","      captions[i] = caption"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JgRyCLrqjueB"},"outputs":[],"source":["# before preprocess of text\n","mapping['1000268201_693b08cb0e']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ZN5XJnwj8WP"},"outputs":[],"source":["# preprocess the text\n","clean(mapping)\n","mapping['1000268201_693b08cb0e']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qjmm2htfkCHO"},"outputs":[],"source":["all_captions = []\n","for key in mapping: \n","  for caption in mapping[key]:\n","    all_captions.append(caption)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zCfzDKHEkrGq"},"outputs":[],"source":["len(all_captions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h1OilkhakslI"},"outputs":[],"source":["all_captions[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O8fe-S96kxKo"},"outputs":[],"source":["# tokenizer the text\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(all_captions)\n","vocab_size = len(tokenizer.word_index) + 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2La_RRVgornM"},"outputs":[],"source":["seq = tokenizer.texts_to_sequences([\"two dogs on pavement moving toward each other\"])[0]\n","seq"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cnKMbZczlHxJ"},"outputs":[],"source":["vocab_size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HzzFTukilIwB"},"outputs":[],"source":["# get maximum length of the caption available\n","max_length = max(len(caption.split()) for caption in all_captions)\n","max_length"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ukyc8OfNlbvK"},"outputs":[],"source":["len(all_captions)"]},{"cell_type":"markdown","metadata":{"id":"AvTo63K1lyqj"},"source":["## Train Test Split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AC0mLRp3lgu6"},"outputs":[],"source":["import random\n","image_ids = list(mapping.keys())\n","split = int(len(image_ids) * 0.9)\n","\n","train = random.sample(image_ids, split)\n","test = [item for item in image_ids if item not in train]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ss5NnTTRl4YU"},"outputs":[],"source":["# startseq girl going into wooden building endsed\n","#    X                                       y\n","# startseq                                  girl\n","# startseq girl                             going\n","# startseq girl going                       into\n","# ...................\n","# startseq girl going to wooden building    endsed\n","\n","# create data generator to get data in batch (avoids session crash)\n","def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n","  # loop over images\n","  X1, X2, y = list(), list(), list()\n","  n = 0\n","  while True: \n","    for key in data_keys:\n","      n += 1\n","      captions = mapping[key]\n","\n","      # process each caption\n","      for caption in captions:\n","        # encode the sequence\n","        seq = tokenizer.texts_to_sequences([caption])[0]\n","\n","        # split the sequence into X, y pairs\n","        for i in range(1, len(seq)):\n","          # split into input and output pairs\n","          in_seq, out_seq = seq[:i], seq[i]\n","\n","          # pad input sequence\n","          in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n","\n","          # encode ouput sequence -> to_category with len of classes = len of vocab\n","          out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n","\n","          X1.append(features[key][0])\n","          X2.append(in_seq)\n","          y.append(out_seq)\n","\n","      if n == batch_size: \n","        X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n","        yield [X1, X2], y\n","\n","        X1, X2, y = list(), list(), list()\n","        n = 0"]},{"cell_type":"markdown","metadata":{"id":"s5_1XTogs9-_"},"source":["## Model Creatation\n","* Following ideal model diagram \\\n","<img src=\"../images/architecture-1.png\">\n","\n","* Detail Model Architecture \\\n","<img src=\"../images/model.png\">"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0uUqE9iAnX6a"},"outputs":[],"source":["# encoder model\n","# image feature layers\n","# inputs1 = Input(shape=(4096, )) # dung voi vgg16\n","inputs1 = Input(shape=(2048, ))\n","fe1 = Dropout(0.4)(inputs1)\n","fe2 = Dense(256, activation=\"relu\")(fe1)\n","\n","# sequence feature layers\n","inputs2 = Input(shape=(max_length, ))\n","se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n","se2 = Dropout(0.4)(se1)\n","se3 = LSTM(256)(se2)\n","\n","# decoder mode\n","decoder1 = add([fe2, se3])\n","decoder2 = Dense(256, activation=\"relu\")(decoder1)\n","outputs = Dense(vocab_size, activation=\"softmax\")(decoder2)\n","\n","model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n","model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n","\n","# plot the model\n","plot_model(model, show_shapes=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TFbsoPmQvMH5"},"outputs":[],"source":["# train the model\n","epochs = 40\n","batch_size = 64\n","steps = len(train) // batch_size\n","\n","for i in range(epochs):\n","  # create data generator\n","  generator = data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)\n","\n","  # fit for one epoch\n","  model.fit(generator, epochs=2, steps_per_epoch=steps, verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x3xOY-bRwN4z"},"outputs":[],"source":["# save the model\n","model.save(os.path.join(MODEL_DIR, \"best_model.h5\"))"]},{"cell_type":"markdown","metadata":{"id":"aJYRjvs2xF2Q"},"source":["## Generate Captions for Image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B8Wkkep8xIWA"},"outputs":[],"source":["# convert index (integer) -> real word\n","def idx_to_word(integer, tokenizer):\n","  for word, index in tokenizer.word_index.items():\n","    if index ==<img src=\"files/subdir/image.png\"> integer: \n","      return word"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FQfOHelJ4piJ"},"outputs":[],"source":["# generate caption for an image\n","def predict_caption(model, image, tokenizer, max_length):\n","  # add start tag for generation process\n","  in_text = 'startseq'\n","\n","  # iterate over the max length of sequence\n","  for i in range(max_length):\n","    # encode input sequence\n","    sequence = tokenizer.texts_to_sequences([in_text])[0]\n","\n","    # pad the sequence\n","    sequence = pad_sequences([sequence], max_length)\n","\n","    # predict n<img src=\"files/subdir/image.png\">ext word\n","    yhat = model.predict([image, sequence], verbose=0)\n","\n","    # get index with hight probability\n","    yhat = np.argmax(yhat)\n","\n","    # covert index to word\n","    word = idx_to_word(yhat, tokenizer)\n","\n","    # stop if word not found\n","    if word is None: \n","      break\n","\n","    # append word as inputs for generating next word\n","    in_text += \" \" + word\n","\n","    # stop if we reach \"endseq\" tag\n","    if word == 'endseq':\n","      break\n","    \n","  return in_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Uq6Q8jS7CGn"},"outputs":[],"source":["<img src=\"files/subdir/image.png\">from nltk.translate.bleu_score import corpus_bleu\n","\n","# validate with test data\n","actual, predicted = list(), list()\n","\n","for key in tqdm(test):\n","  # get actual caption\n","  captions = mapping[key]\n","\n","  # predict the caption for image\n","  y_pred = predict_caption(model, features[key], tokenizer, max_length)\n","\n","  # split into words\n","  actual_captions = [caption.split() for caption in captions]\n","  y_pred = y_pred.split()\n","\n","  # append to the list\n","  actual.append(actual_captions)\n","  predicted.append(y_pred)\n","\n","# Calculate BLEU score\n","print(f\"BLEU-1: {corpus_bleu(actual, predicted, weights=(1.0, 0.0, 0.0))}\")\n","print(f\"BLEU-2: {corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0.0))}\")"]},{"cell_type":"markdown","metadata":{"id":"d6xHcrb79oJT"},"source":["## Visualize the Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yTjj2MCD9k38"},"outputs":[],"source":["from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","def plot_generate_caption(image_name):\n","  # load the image\n","  # image_name = \"1001773457_577c3a7d70.jpg\"\n","  image_id = image_name.split('.')[0]\n","  img_path = os.path.join(BASE_DIR, \"Images\", image_name)\n","  image = Image.open(img_path)\n","  captions = mapping[image_id]\n","\n","  print('---------------------Actual---------------------')\n","  for caption in captions:\n","    print(caption)\n","\n","  # predict the caption\n","  y_pred = predict_caption(model, features[image_id], tokenizer, max_length)\n","  print('--------------------Predicted--------------------')\n","  print(y_pred)\n","  \n","  plt.imshow(image)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4BBL-BfY_2zu"},"outputs":[],"source":["plot_generate_caption(\"1030985833_b0902ea560.jpg\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"scJve3CK_8aN"},"outputs":[],"source":["plot_generate_caption(\"1055753357_4fa3d8d693.jpg\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZWTeW6HdAj4L"},"outputs":[],"source":["plot_generate_caption(\"109738916_236dc456ac.jpg\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hQNMj3QTAoxs"},"outputs":[],"source":["# should change <start> -> startseq and <end> -> endseq"]},{"cell_type":"markdown","metadata":{"id":"rVOXu8k6qpW7"},"source":["## NEXT"]},{"cell_type":"markdown","metadata":{"id":"Eyfk1mqhq-kL"},"source":["# Dataset Information\n","\n","Develop a Deep Learning program to identify when an article might be fake news\n","\n","## Attributes\n","* id: unique id for a new article\n","* title: the title of a news article\n","* author: author of the news article\n","* text: the text of the article; could be incomplete\n","* label: a label that marks the article as potentially unrealiable\n","  + 1: unrealiable\n","  + 0: realiable\n","\n","\n","* source ref: https://github.com/aswintechguy/Deep-Learning-Projects/tree/main/Fake%20News%20Detection%20Analysis%20-%20LSTM%20Classification"]},{"cell_type":"markdown","metadata":{"id":"DFFL-93nrJ6q"},"source":["## Import Modules"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"srh5pKb7qqj3"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import re\n","import nltk\n","import warnings\n","\n","%matplotlib inline\n","\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"RGUae3JMstIa"},"source":["## Loading the Dataset\n","\n","* URL dataset: https://www.kaggle.com/c/fake-news/data?select=test.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yo9hHxZJsxdE"},"outputs":[],"source":["df = pd.read_csv(\"train.csv\")\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XVSdXFVos7CF"},"outputs":[],"source":["df[\"title\"][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oDPfbfQ7tvGM"},"outputs":[],"source":["df[\"text\"][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eQVgOJQdtxCj"},"outputs":[],"source":["df.info()"]},{"cell_type":"markdown","metadata":{"id":"zeV1JUsnt52I"},"source":["## Data Processing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"73cJwpCvuRhq"},"outputs":[],"source":["# drop unnessary columns\n","df = df.drop(columns=['id', 'title', 'author'], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IjftaEi3t78x"},"outputs":[],"source":["# drop null values\n","df = df.dropna(axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dByuWZuRurC6"},"outputs":[],"source":["# remove special characters and punctuations\n","df['clean_news'] = df['text'].str.lower()\n","df['clean_news']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vGcNdh8qu2zW"},"outputs":[],"source":["df['clean_news'] = df['clean_news'].str.replace(\"[^A-Za-z0-9\\s']\", \"\")\n","df['clean_news'] = df['clean_news'].str.replace(\"\\n\", \"\")\n","df['clean_news'] = df['clean_news'].str.replace(\"\\s+\", \" \")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4jK8383Xv22K"},"outputs":[],"source":["# remove stop word\n","from nltk.corpus import stopwords\n","stop = stopwords.words('english')\n","\n","df['clean_news'] = df['clean_news'].apply(lambda x: \" \".join([word for word in x.split() if word not in stop]))\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"U_I97C3bwncK"},"source":["## Exploratory Data Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A_XFpD4txRMC"},"outputs":[],"source":["from wordcloud import WordCloud"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LRm1bzyAwp7X"},"outputs":[],"source":["# visualize the frequent words\n","all_words = \" \".join([sentence for sentence in df['clean_news']])\n","\n","wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_words)\n","\n","# plot the graph \n","plt.figure(figsize=(15, 9))\n","plt.imshow(wordcloud, interpolation=\"bilinear\")\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VrL4GI9Wx3o_"},"outputs":[],"source":["# visualize the frequenct words for realiable news\n","all_words = \" \".join([sentence for sentence in df[\"clean_news\"][df[\"label\"] == 0]])\n","\n","wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_words)\n","\n","# plot the graph\n","plt.figure(figsize=(15, 9))\n","plt.imshow(wordcloud, interpolation=\"bilinear\")\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kQtE4IFJyTjn"},"outputs":[],"source":["# visualize the frequenct words for realiable news\n","all_words = \" \".join([sentence for sentence in df[\"clean_news\"][df[\"label\"] == 1]])\n","\n","wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_words)\n","\n","# plot the graph\n","plt.figure(figsize=(15, 9))\n","plt.imshow(wordcloud, interpolation=\"bilinear\")\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"X9YsraBAywGl"},"source":["## Create Word Embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N_VUE6Vayv1j"},"outputs":[],"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BffFrjNLzGk9"},"outputs":[],"source":["# tokenize text\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(df['clean_news'])\n","\n","word_index = tokenizer.word_index\n","vocab_size = len(word_index)\n","vocab_size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DSd_7INgzXHZ"},"outputs":[],"source":["# padding data\n","sequences = tokenizer.texts_to_sequences(df[\"clean_news\"])\n","padded_seq = pad_sequences(sequences, maxlen=700, padding=\"post\", truncating=\"post\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1MM11Mw00FRN"},"outputs":[],"source":["# create embedding index\n","embedding_index = {}\n","\n","with open('glove.6B.100d.txt', encoding='utf-8') as f:\n","  for line in f: \n","    values = line.split()\n","\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype=np.float32)\n","    embedding_index[word] = coefs\n","\n","    # print(len(coefs)) # coefs with length 100\n","    # break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MUOcko2R10ln"},"outputs":[],"source":["# create embedding matrix\n","embedding_matrix = np.zeros((vocab_size+1, 100))\n","for word, i in word_index.items():\n","  embedding_vector = embedding_index.get(word)\n","\n","  if embedding_vector is not None: \n","    embedding_matrix[i] = embedding_vector"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oDMTVIXf2BxJ"},"outputs":[],"source":["embedding_matrix[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tcSpde-d1_H5"},"outputs":[],"source":["embedding_matrix[1]"]},{"cell_type":"markdown","metadata":{"id":"NPZl-PG8ymGG"},"source":["## Input Split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j-gWhjXrynzL"},"outputs":[],"source":["padded_seq[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AmCX78NA2R04"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(padded_seq, df[\"label\"], test_size=0.2, random_state=42, stratify=df[\"label\"])\n"]},{"cell_type":"markdown","metadata":{"id":"y7APHYhq23yU"},"source":["## Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fsFABL6L25ZX"},"outputs":[],"source":["from tensorflow.keras.layers import LSTM, Dropout, Dense, Embedding\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","\n","model = Sequential([\n","    Embedding(vocab_size+1, 100, weights=[embedding_matrix], trainable=False), \n","    Dropout(0.2), \n","    LSTM(128, return_sequences=True),\n","    LSTM(128), \n","    Dropout(0.2), \n","    Dense(512), \n","    Dropout(0.2), \n","    Dense(256), \n","    Dropout(0.2), \n","    Dense(1, activation=\"sigmoid\")\n","\n","])\n","\n","# create checkpoint\n","path_checkpoint = \"./checkpoint.ckpt\"\n","callback = ModelCheckpoint(filepath=path_checkpoint, save_weights_only=True, verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TikIr1LY30xf"},"outputs":[],"source":["model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bf4ZmuWs4GJR"},"outputs":[],"source":["# train the model\n","history = model.fit(\n","    X_train, y_train, \n","    epochs=10, \n","    batch_size=128, \n","    validation_data=[X_test, y_test], \n","    callbacks=[callback]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UGZ_QZxT8Cjb"},"outputs":[],"source":["# save model\n","model.save(\"./best_model.h5\")\n","\n","# load model C1\n","# 1. create instan model | 2. load model\n","# model = create_model()\n","# model.load(\"./best_model.h5\")\n","\n","# load model C2: \n","# new_model = tf.keras.models.load_model(\"./best_model.h5\")\n","\n","# model save_weights\n","# model.save_weights(\"./save_weight.ckpt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J5wOXoa_7v16"},"outputs":[],"source":["# load model from checkpoint_weight\n","\n","# create a new model instance\n","# model.save_weights()\n","# model = create_model()\n","# model.load_weights(path_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OsfWYCnY4eXQ"},"outputs":[],"source":["# visualize the results\n","plt.plot(history.history[\"accuracy\"])\n","plt.plot(history.history[\"val_accuracy\"])\n","plt.xlabel('epochs')\n","plt.ylabel('accuracy')\n","plt.legend([\"Train\", \"Test\"])\n","plt.show()\n","\n","plt.plot(history.history[\"loss\"])\n","plt.plot(history.history[\"loss\"])\n","plt.xlabel('epochs')\n","plt.ylabel('loss')\n","plt.legend([\"Train\", \"Test\"])\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOIn7/NjWQFgmJ9BTtUGq6X","collapsed_sections":[],"mount_file_id":"1jsH4yTl4Q-vM1yCGfrzVhG3OU7nDyReS","name":"source.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.7.13 ('ts')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"vscode":{"interpreter":{"hash":"442f670e5fde21ceb3fbec614a62c699aa168f99318a78112edb0d432083c0c2"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"0406ce95b3594820b977a0ead3d68b0f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b7bdd13b2f948fa840cbe343c44943a","placeholder":"​","style":"IPY_MODEL_826eea88f1974155a13b572a9ac00ba9","value":" 84%"}},"13d2fda0fc504c22923a8a32c01957aa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab39904669e94113b8180fc6aa546fc0","placeholder":"​","style":"IPY_MODEL_749d77007d514161bb872967c1c4f7a2","value":" 6801/8091 [1:34:26&lt;17:32,  1.23it/s]"}},"3ca99a81cc344f438013c41289fc3f11":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_95f4088b75014615bc4d68f678bb2bc5","max":8091,"min":0,"orientation":"horizontal","style":"IPY_MODEL_59985f5377934236804742f982c64797","value":6801}},"59985f5377934236804742f982c64797":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6b7bdd13b2f948fa840cbe343c44943a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"749d77007d514161bb872967c1c4f7a2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"826eea88f1974155a13b572a9ac00ba9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"95f4088b75014615bc4d68f678bb2bc5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab39904669e94113b8180fc6aa546fc0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c05be3f3bda34c14a1c0864523e11b8b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da4428797a1745cd8812fd5680aeaa4c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0406ce95b3594820b977a0ead3d68b0f","IPY_MODEL_3ca99a81cc344f438013c41289fc3f11","IPY_MODEL_13d2fda0fc504c22923a8a32c01957aa"],"layout":"IPY_MODEL_c05be3f3bda34c14a1c0864523e11b8b"}}}}},"nbformat":4,"nbformat_minor":0}
